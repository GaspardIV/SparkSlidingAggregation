{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Implementation of minimal map reduce SLIDING AGGREGATION https://dl.acm.org/doi/10.1145/2463676.2463719\n",
    "\n",
    "https://www.cse.cuhk.edu.hk/~taoyf/paper/sigmod13-mr.pdf\n",
    "\n",
    "Authors of algortithm: Yufei Tao, Wenqing Lin, Xiaokui Xiao"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Yellow Taxi Trip Records (CSV) data from https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page for January 2021. For each record I've computed the average ride distance and the average passenger occupancy during the last 1000 rides. The algorithm is minimal and follows the one from the paper. It Uses Spark RDD API Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto",
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    },
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# config\n",
    "\n",
    "FILE = \"yellow_tripdata_2021-01.csv\"\n",
    "COL_NAMES = [\"tpep_dropoff_datetime\", \"passenger_count\", \"trip_distance\"]\n",
    "AGGREGATE_FIELD = \"passenger_count\"\n",
    "# AGGREGATE_FIELD = \"trip_distance\"\n",
    "AGGREGATE_COLUMN_NAME = \"aggregate_result\"\n",
    "SORT_FIELD = \"tpep_dropoff_datetime\"\n",
    "t = 8  # num of machines\n",
    "l = 1000  # length of window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "autoscroll": "auto",
    "pycharm": {
     "name": "#%%\n"
    },
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# globals\n",
    "\n",
    "import random\n",
    "from math import ceil\n",
    "from math import log\n",
    "from operator import add\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "DUMMY = 'DUMMY'\n",
    "SUM_FROM_OTHER_PREFIX = 'WHOLE_SUM_FROM_'\n",
    "OBJECTS_FROM_OTHER_PREFIX = 'OBJECTS_FROM_'\n",
    "RANK_SUM_PREFIX = 'BIG_PREFIX_'\n",
    "RANK_KEY = 'rank'\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "m = 0\n",
    "n = 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "autoscroll": "auto",
    "pycharm": {
     "name": "#%%\n"
    },
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# utlis\n",
    "\n",
    "def get_key_value(x):\n",
    "    return x[SORT_FIELD]\n",
    "\n",
    "\n",
    "def get_agg_value(x):\n",
    "    try:\n",
    "        return float(x[AGGREGATE_FIELD])\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def is_dummy(x):\n",
    "    return get_key_value(x) == DUMMY\n",
    "\n",
    "\n",
    "def decision(row, probability):\n",
    "    if is_dummy(row):\n",
    "        return False\n",
    "    return random.random() < probability\n",
    "\n",
    "\n",
    "def compute_rho(m, n, t):\n",
    "    return 1 / m * log(n * t)\n",
    "\n",
    "\n",
    "def key(x):\n",
    "    return x[0]\n",
    "\n",
    "\n",
    "def value(x):\n",
    "    return x[1]\n",
    "\n",
    "\n",
    "def rank(o):\n",
    "    return o[RANK_KEY]\n",
    "\n",
    "\n",
    "class RankRangeSum:\n",
    "    def __init__(self, array, start_rank):\n",
    "        self.prefix_sum = [0]\n",
    "        self.start_rank = start_rank\n",
    "        for x in array:\n",
    "            self.prefix_sum.append(x + self.prefix_sum[-1])\n",
    "\n",
    "    def get_sum_from_rank_range(self, i, j):\n",
    "        if j + 1 - self.start_rank >= len(self.prefix_sum):\n",
    "            return float(self.prefix_sum[-1] - self.prefix_sum[max(i - self.start_rank, 0)])\n",
    "        return float(self.prefix_sum[j + 1 - self.start_rank] - self.prefix_sum[max(i - self.start_rank, 0)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "autoscroll": "auto",
    "pycharm": {
     "name": "#%%\n"
    },
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# data prepare + sampling for terasort\n",
    "\n",
    "def add_dummies(df):\n",
    "    global n\n",
    "    if n % t != 0:\n",
    "        dummies_count_to_add = t - n % t\n",
    "        dummies_to_add = [[DUMMY for _ in COL_NAMES] for _ in range(dummies_count_to_add)]\n",
    "        df = df.union(spark.createDataFrame(dummies_to_add))\n",
    "        n = df.count()\n",
    "    return df\n",
    "\n",
    "\n",
    "def prepare_rdd():\n",
    "    global m, n\n",
    "    df = spark.read.options(sep=\",\", header=True).csv(FILE).select(COL_NAMES)\n",
    "    n = df.count()\n",
    "    df = add_dummies(df)\n",
    "    rdd = df.rdd.repartition(t)\n",
    "    m = n / t\n",
    "    return rdd\n",
    "\n",
    "\n",
    "def sample(rdd, rho):\n",
    "    res = rdd.map(lambda x: x if decision(x, rho) else None).filter(lambda x: x is not None).collect()\n",
    "    res = sorted(res, key=lambda x: get_key_value(x))\n",
    "    return res\n",
    "\n",
    "\n",
    "def get_boundaries_from_sample(sample):\n",
    "    boundaries = []\n",
    "    for i in range(1, t):\n",
    "        index = i * ceil(len(sample) / t) - 1\n",
    "        boundaries.append(sample[index])\n",
    "\n",
    "    return boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "autoscroll": "auto",
    "pycharm": {
     "name": "#%%\n"
    },
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# tera sort\n",
    "\n",
    "def tera_sort(rdd):\n",
    "    rho = compute_rho(m, n, t)\n",
    "    boundaries = get_boundaries_from_sample(sample(rdd, rho))\n",
    "    # In the paper it's sent to machines directly like 'RANK_SUM_PREFIX' in rank\n",
    "    boundaries = spark.sparkContext.broadcast(boundaries)\n",
    "\n",
    "    def map_boundaries(x):\n",
    "        for idx, boundary in enumerate(boundaries.value):\n",
    "            if get_key_value(x) <= get_key_value(boundary):\n",
    "                return idx, [x]\n",
    "        return len(boundaries.value), [x]\n",
    "\n",
    "    res = rdd.map(map_boundaries).reduceByKey(add).map(lambda x: (key(x), sorted(value(x), key=get_key_value)))\n",
    "    boundaries.unpersist()\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "autoscroll": "auto",
    "pycharm": {
     "name": "#%%\n"
    },
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# ranking and perfect balance\n",
    "\n",
    "def do_rank(sorted_rdd):\n",
    "    def mapper(v):\n",
    "        return [v] + [(receiver, [(RANK_SUM_PREFIX + str(key(v)), len(value(v)))]) for receiver in\n",
    "                      range(key(v) + 1, t)]\n",
    "\n",
    "    def compute_rank(x):\n",
    "        big_prefixes_sum = sum(map(lambda prefix_tuple: value(prefix_tuple),\n",
    "                                   filter(lambda el: key(el).startswith(RANK_SUM_PREFIX), value(x))))\n",
    "        elements = list(filter(lambda el: not key(el).startswith(RANK_SUM_PREFIX), value(x)))\n",
    "        for idx, el in enumerate(elements):\n",
    "            elements[idx] = el.asDict()\n",
    "            elements[idx][RANK_KEY] = idx + big_prefixes_sum + 1\n",
    "        return x[0], elements\n",
    "\n",
    "    res = sorted_rdd.flatMap(mapper).reduceByKey(add).map(compute_rank)\n",
    "    return res\n",
    "\n",
    "\n",
    "def perfect_balance(ranked_rdd):\n",
    "    res = ranked_rdd.flatMap(lambda x: value(x)).map(lambda x: (ceil(rank(x) / m - 1), [x])).reduceByKey(add).map(\n",
    "        lambda x: (int(key(x)), sorted(value(x), key=get_key_value)))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "autoscroll": "auto",
    "pycharm": {
     "name": "#%%\n"
    },
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# sliding aggregation\n",
    "\n",
    "def sliding_agregation(balanced_rdd):\n",
    "    def send_objects_to_other_machines(v):\n",
    "        whole_sums = [(i, [(SUM_FROM_OTHER_PREFIX + str(v[0]), sum(map(get_agg_value, v[1])))]) for i in range(v[0]+1, t)]\n",
    "        objects_to_send = []\n",
    "        if l <= m:\n",
    "            if key(v) + 1 < t:\n",
    "                objects_to_send = [(key(v) + 1, [(OBJECTS_FROM_OTHER_PREFIX + str(key(v)), value(v))])]\n",
    "        else:\n",
    "            receiver1 = int(key(v) + (l - 1) // m)\n",
    "            receiver2 = receiver1 + 1\n",
    "            if receiver1 < t:\n",
    "                objects_to_send = objects_to_send + [(receiver1, [(OBJECTS_FROM_OTHER_PREFIX + str(key(v)), value(v))])]\n",
    "            if receiver2 < t:\n",
    "                objects_to_send = objects_to_send + [(receiver2, [(OBJECTS_FROM_OTHER_PREFIX + str(key(v)), value(v))])]\n",
    "        return objects_to_send + whole_sums + [v]\n",
    "\n",
    "    def compute_windows(x):\n",
    "        (i, value) = x\n",
    "        own_objects = list(filter(lambda el: type(el) is dict, value))\n",
    "        without_own_objects = list(filter(lambda els: type(els) is not dict, value))\n",
    "        objects_from_others = dict(map(lambda els: (int(els[0][len(OBJECTS_FROM_OTHER_PREFIX):]), els[1]),\n",
    "                                       filter(lambda els: els[0].startswith(OBJECTS_FROM_OTHER_PREFIX),\n",
    "                                              without_own_objects)))\n",
    "        whole_sums = dict(map(lambda els: (int(els[0][len(SUM_FROM_OTHER_PREFIX):]), els[1]),\n",
    "                              filter(lambda els: els[0].startswith(SUM_FROM_OTHER_PREFIX), without_own_objects)))\n",
    "        prefix_sum_own_objects = RankRangeSum(map(get_agg_value, own_objects), rank(own_objects[0]))\n",
    "\n",
    "        prefix_sums_of_others_objects = dict(\n",
    "            map(lambda item: (item[0], RankRangeSum(map(get_agg_value, item[1]), rank(item[1][0]))),\n",
    "                objects_from_others.items()))\n",
    "\n",
    "        for o in own_objects:\n",
    "            window_o = 0\n",
    "            to_rank = rank(o)\n",
    "            from_rank = to_rank - l + 1\n",
    "            alpha = ceil(from_rank / m) - 1\n",
    "\n",
    "            if alpha >= 0 and alpha != i:\n",
    "                window_o += prefix_sums_of_others_objects[alpha].get_sum_from_rank_range(from_rank, to_rank)\n",
    "\n",
    "            if alpha != i:\n",
    "                window_o += sum(map(lambda x: x[1], filter(lambda x: alpha < x[0] < i, whole_sums.items())))\n",
    "\n",
    "            window_o += prefix_sum_own_objects.get_sum_from_rank_range(from_rank, to_rank)\n",
    "\n",
    "            o[AGGREGATE_COLUMN_NAME] = window_o / min(rank(o), l)\n",
    "\n",
    "        own_objects = list(filter(lambda x: not is_dummy(x), own_objects))\n",
    "        return own_objects\n",
    "\n",
    "    res = balanced_rdd.flatMap(send_objects_to_other_machines).reduceByKey(add).flatMap(compute_windows)\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "autoscroll": "auto",
    "pycharm": {
     "name": "#%%\n"
    },
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "%spark.pyspark\n",
    "\n",
    "# main\n",
    "\n",
    "def execute_sliding_window():\n",
    "    rdd = prepare_rdd()\n",
    "    sorted_rdd = tera_sort(rdd)\n",
    "    ranked_rdd = do_rank(sorted_rdd)\n",
    "    balanced_rdd = perfect_balance(ranked_rdd)\n",
    "    aggregated_rdd = sliding_agregation(balanced_rdd)\n",
    "    result = aggregated_rdd.collect()\n",
    "    df = spark.createDataFrame(result)\n",
    "    df.show(1200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "autoscroll": "auto",
    "pycharm": {
     "name": "#%%\n"
    },
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "execute_sliding_window()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "autoscroll": "auto",
    "pycharm": {
     "name": "#%%\n"
    },
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "AGGREGATE_FIELD = \"trip_distance\"\n",
    "execute_sliding_window()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "autoscroll": "auto",
    "pycharm": {
     "name": "#%%\n"
    },
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "language": "scala",
   "name": "spark2-scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala"
  },
  "name": "pdd_zad1_kk385785"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
